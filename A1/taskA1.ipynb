{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import image_utils\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# PATH TO ALL IMAGES\n",
    "global basedir, image_paths, target_size\n",
    "basedir = os.path.dirname(os.path.abspath(''))\n",
    "dataset_dir = os.path.join(basedir,'Datasets')\n",
    "images_dir = os.path.join(dataset_dir,'celeba')\n",
    "test_images_dir = os.path.join(dataset_dir,'celeba_test')\n",
    "predictor_dir = os.path.join(basedir,'shape_predictor_68_face_landmarks.dat')\n",
    "labels_filename = 'labels.csv'\n",
    "\n",
    "\n",
    "\n",
    "# how to find frontal human faces in an image using 68 landmarks.  These are points on the face such as the corners of the mouth, along the eyebrows, on the eyes, and so forth.\n",
    "\n",
    "# The face detector we use is made using the classic Histogram of Oriented\n",
    "# Gradients (HOG) feature combined with a linear classifier, an image pyramid,\n",
    "# and sliding window detection scheme.  The pose estimator was created by\n",
    "# using dlib's implementation of the paper:\n",
    "# One Millisecond Face Alignment with an Ensemble of Regression Trees by\n",
    "# Vahid Kazemi and Josephine Sullivan, CVPR 2014\n",
    "# and was trained on the iBUG 300-W face landmark dataset (see https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/):\n",
    "#     C. Sagonas, E. Antonakos, G, Tzimiropoulos, S. Zafeiriou, M. Pantic.\n",
    "#     300 faces In-the-wild challenge: Database and results.\n",
    "#     Image and Vision Computing (IMAVIS), Special Issue on Facial Landmark Localisation \"In-The-Wild\". 2016.\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_dir)\n",
    "\n",
    "\n",
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((shape.num_parts, 2), dtype=dtype)\n",
    "\n",
    "    # loop over all facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, shape.num_parts):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords\n",
    "\n",
    "def rect_to_bb(rect):\n",
    "    # take a bounding predicted by dlib and convert it\n",
    "    # to the format (x, y, w, h) as we would normally do\n",
    "    # with OpenCV\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    "\n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)\n",
    "\n",
    "\n",
    "def run_dlib_shape(image):\n",
    "    # in this function we load the image, detect the landmarks of the face, and then return the image and the landmarks\n",
    "    # load the input image, resize it, and convert it to grayscale\n",
    "    resized_image = image.astype('uint8')\n",
    "\n",
    "    gray = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    gray = gray.astype('uint8')\n",
    "\n",
    "    # detect faces in the grayscale image\n",
    "    rects = detector(gray, 1)\n",
    "    num_faces = len(rects)\n",
    "\n",
    "    if num_faces == 0:\n",
    "        return None, resized_image\n",
    "\n",
    "    face_areas = np.zeros((1, num_faces))\n",
    "    face_shapes = np.zeros((136, num_faces), dtype=np.int64)\n",
    "\n",
    "    # loop over the face detections\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "        # array\n",
    "        temp_shape = predictor(gray, rect)\n",
    "        temp_shape = shape_to_np(temp_shape)\n",
    "\n",
    "        # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "        # [i.e., (x, y, w, h)],\n",
    "        #   (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        (x, y, w, h) = rect_to_bb(rect)\n",
    "        face_shapes[:, i] = np.reshape(temp_shape, [136])\n",
    "        face_areas[0, i] = w * h\n",
    "    # find largest face and keep\n",
    "    dlibout = np.reshape(np.transpose(face_shapes[:, np.argmax(face_areas)]), [68, 2])\n",
    "\n",
    "    return dlibout, resized_image\n",
    "\n",
    "def extract_features_labels(test = False):\n",
    "    \"\"\"\n",
    "    This funtion extracts the landmarks features for all images in the folder 'dataset/celeba'.\n",
    "    It also extract the gender label for each image.\n",
    "    :return:\n",
    "        landmark_features:  an array containing 68 landmark points for each image in which a face was detected\n",
    "        gender_labels:      an array containing the gender label (male=0 and female=1) for each image in\n",
    "                            which a face was detected\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Getting train images\")\n",
    "    labels_df = pd.read_csv(os.path.join(images_dir, labels_filename), sep='\\t')\n",
    "    image_paths = [os.path.join(images_dir, \"img\", img_name) for img_name in labels_df['img_name'].values]\n",
    "\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    gender_labels = labels_df['gender'].values\n",
    "    if os.path.isdir(images_dir):\n",
    "        \n",
    "        for i, img_path in enumerate(image_paths):\n",
    "            # load image\n",
    "            img = image_utils.img_to_array(\n",
    "                image_utils.load_img(img_path,\n",
    "                               target_size=None,\n",
    "                               interpolation='bicubic'))\n",
    "            features, _ = run_dlib_shape(img)\n",
    "            if features is not None:\n",
    "                all_features.append(features)\n",
    "                all_labels.append(gender_labels[i])\n",
    "\n",
    "    print(\"Getting test images\")\n",
    "    labels_df = pd.read_csv(os.path.join(test_images_dir, labels_filename), sep='\\t')\n",
    "    image_paths = [os.path.join(test_images_dir, \"img\", img_name) for img_name in labels_df['img_name'].values]\n",
    "\n",
    "    gender_labels = labels_df['gender'].values\n",
    "    if os.path.isdir(images_dir):\n",
    "        \n",
    "        for i, img_path in enumerate(image_paths):\n",
    "            # load image\n",
    "            img = image_utils.img_to_array(\n",
    "                image_utils.load_img(img_path,\n",
    "                               target_size=None,\n",
    "                               interpolation='bicubic'))\n",
    "            features, _ = run_dlib_shape(img)\n",
    "            if features is not None:\n",
    "                all_features.append(features)\n",
    "                all_labels.append(gender_labels[i])\n",
    "\n",
    "    landmark_features = np.array(all_features)\n",
    "    gender_labels = (np.array(all_labels) + 1)/2 # simply converts the -1 into 0, so male=0 and female=1\n",
    "    return landmark_features, gender_labels\n",
    "\n",
    "def get_data():\n",
    "    X, y = extract_features_labels()\n",
    "    Y = np.array(y).T\n",
    "    return X, Y\n",
    "\n",
    "def img_SVM_with_cv(training_images, training_labels):\n",
    "    print(\"SVM with Cross-Validation\")\n",
    "    classifier = SVC(kernel='linear')\n",
    "    cv_scores = cross_val_score(classifier, training_images, training_labels, cv=5)\n",
    "    print(\"Cross-Validation Scores:\", cv_scores)\n",
    "    print(\"Mean Score:\", np.mean(cv_scores))\n",
    "    print(\"Standard Deviation:\", np.std(cv_scores))\n",
    "\n",
    "def img_logistic_regression_with_cv(training_images, training_labels):\n",
    "    print(\"Logistic Regression with Cross-Validation\")\n",
    "    classifier = LogisticRegression(max_iter=5000)\n",
    "    cv_scores = cross_val_score(classifier, training_images, training_labels, cv=5)\n",
    "    print(\"Cross-Validation Scores:\", cv_scores)\n",
    "    print(\"Mean Score:\", np.mean(cv_scores))\n",
    "    print(\"Standard Deviation:\", np.std(cv_scores))\n",
    "\n",
    "\n",
    "\n",
    "def img_random_forest_with_cv(training_images, training_labels):\n",
    "    print(\"Random Forest with Cross-Validation\")\n",
    "    classifier = RandomForestClassifier()\n",
    "    cv_scores = cross_val_score(classifier, training_images, training_labels, cv=5)\n",
    "    print(\"Cross-Validation Scores:\", cv_scores)\n",
    "    print(\"Mean Score:\", np.mean(cv_scores))\n",
    "    print(\"Standard Deviation:\", np.std(cv_scores))\n",
    "\n",
    "def img_decision_tree_with_cv(training_images, training_labels):\n",
    "    print(\"Decision Tree with Cross-Validation\")\n",
    "    classifier = DecisionTreeClassifier()\n",
    "    cv_scores = cross_val_score(classifier, training_images, training_labels, cv=5)\n",
    "    print(\"Cross-Validation Scores:\", cv_scores)\n",
    "    print(\"Mean Score:\", np.mean(cv_scores))\n",
    "    print(\"Standard Deviation:\", np.std(cv_scores))\n",
    "\n",
    "def img_mlp_with_cv(training_images, training_labels):\n",
    "    print(\"MLP with Cross-Validation\")\n",
    "    classifier = MLPClassifier()\n",
    "    cv_scores = cross_val_score(classifier, training_images, training_labels, cv=5)\n",
    "    print(\"Cross-Validation Scores:\", cv_scores)\n",
    "    print(\"Mean Score:\", np.mean(cv_scores))\n",
    "    print(\"Standard Deviation:\", np.std(cv_scores))\n",
    "\n",
    "def img_naive_bayes_with_cv(training_images, training_labels):\n",
    "    print(\"Naive Bayes with Cross-Validation\")\n",
    "    classifier = GaussianNB()\n",
    "    cv_scores = cross_val_score(classifier, training_images, training_labels, cv=5)\n",
    "    print(\"Cross-Validation Scores:\", cv_scores)\n",
    "    print(\"Mean Score:\", np.mean(cv_scores))\n",
    "    print(\"Standard Deviation:\", np.std(cv_scores))\n",
    "\n",
    "def img_SVM_with_randomized_search(training_images, training_labels):\n",
    "    print(\"SVM with Randomized Search\")\n",
    "    classifier = SVC(kernel='linear')\n",
    "    param_distributions = {'C': np.logspace(-3, 3, 7), 'kernel': ['linear']}\n",
    "    random_search = RandomizedSearchCV(classifier, param_distributions, cv=5, n_iter=10)\n",
    "    random_search.fit(training_images, training_labels)\n",
    "    print(\"Best Parameters:\", random_search.best_params_)\n",
    "    print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "def img_logistic_regression_with_randomized_search(training_images, training_labels):\n",
    "    print(\"Logistic Regression with Randomized Search\")\n",
    "    classifier = LogisticRegression(max_iter=5000)\n",
    "    param_distributions = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\n",
    "    random_search = RandomizedSearchCV(classifier, param_distributions, cv=5, n_iter=10)\n",
    "    random_search.fit(training_images, training_labels)\n",
    "    print(\"Best Parameters:\", random_search.best_params_)\n",
    "    print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "def img_random_forest_with_randomized_search(training_images, training_labels):\n",
    "    print(\"Random Forest with Randomized Search\")\n",
    "    classifier = RandomForestClassifier()\n",
    "    param_distributions = {'n_estimators': np.arange(10, 100, 10), 'max_depth': np.arange(1, 11)}\n",
    "    random_search = RandomizedSearchCV(classifier, param_distributions, cv=5, n_iter=10)\n",
    "    random_search.fit(training_images, training_labels)\n",
    "    print(\"Best Parameters:\", random_search.best_params_)\n",
    "    print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "def img_decision_tree_with_randomized_search(training_images, training_labels):\n",
    "    print(\"Decision Tree with Randomized Search\")\n",
    "    classifier = DecisionTreeClassifier()\n",
    "    param_distributions = {'max_depth': np.arange(1, 11)}\n",
    "    random_search = RandomizedSearchCV(classifier, param_distributions, cv=5, n_iter=10)\n",
    "    random_search.fit(training_images, training_labels)\n",
    "    print(\"Best Parameters:\", random_search.best_params_)\n",
    "    print(\"Best Score:\", random_search.best_score_)\n",
    "\n",
    "def img_KNN_with_randomized_search(training_images, training_labels):\n",
    "    print(\"K-Nearest Neighbors with Randomized Search\")\n",
    "    classifier = KNeighborsClassifier()\n",
    "    param_distributions = {'n_neighbors': np.arange(1, 30), 'weights': ['uniform', 'distance']}\n",
    "    random_search = RandomizedSearchCV(classifier, param_distributions, cv=5, n_iter=10)\n",
    "    random_search.fit(training_images, training_labels)\n",
    "    print(\"Best Parameters:\", random_search.best_params_)\n",
    "    print(\"Best Score:\", random_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting train images\n",
      "Getting test images\n"
     ]
    }
   ],
   "source": [
    "tr_X, tr_Y = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with Cross-Validation\n",
      "Cross-Validation Scores: [0.9237435  0.9237435  0.91421144 0.91673894 0.91847355]\n",
      "Mean Score: 0.9193821858733378\n",
      "Standard Deviation: 0.003810278952139287\n"
     ]
    }
   ],
   "source": [
    "img_SVM_with_cv(tr_X.reshape((tr_X.shape[0], 68*2)), tr_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with Cross-Validation\n",
      "Cross-Validation Scores: [0.92461005 0.92287695 0.91681109 0.92367736 0.91934085]\n",
      "Mean Score: 0.9214632613887966\n",
      "Standard Deviation: 0.0029308826397779417\n"
     ]
    }
   ],
   "source": [
    "img_logistic_regression_with_cv(tr_X.reshape((tr_X.shape[0], 68*2)), tr_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with Cross-Validation\n",
      "Cross-Validation Scores: [0.88561525 0.85788562 0.85701906 0.88117953 0.86123157]\n",
      "Mean Score: 0.8685862064300649\n",
      "Standard Deviation: 0.012255385116915167\n"
     ]
    }
   ],
   "source": [
    "img_random_forest_with_cv(tr_X.reshape((tr_X.shape[0], 68*2)), tr_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP with Cross-Validation\n",
      "Cross-Validation Scores: [0.6117851  0.49566724 0.59445407 0.50563747 0.65568083]\n",
      "Mean Score: 0.5726449425130132\n",
      "Standard Deviation: 0.06215805095425741\n"
     ]
    }
   ],
   "source": [
    "img_mlp_with_cv(tr_X.reshape((tr_X.shape[0], 68*2)), tr_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Cross-Validation\n",
      "Cross-Validation Scores: [0.69584055 0.68284229 0.81109185 0.68083261 0.68430182]\n",
      "Mean Score: 0.7109818257247691\n",
      "Standard Deviation: 0.05032674705412573\n"
     ]
    }
   ],
   "source": [
    "img_naive_bayes_with_cv(tr_X.reshape((tr_X.shape[0], 68*2)), tr_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with Randomized Search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moshe\\anaconda3\\envs\\AML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 7 is smaller than n_iter=10. Running 7 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "img_SVM_with_randomized_search(tr_X.reshape((tr_X.shape[0], 136)), tr_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_SVM_with_cv(tr_X.reshape((tr_X.shape[0], 68*2)), tr_Y)\n",
    "# print(\" \")\n",
    "# img_SVM_with_randomized_search(tr_X.reshape((tr_X.shape[0], 136)), tr_Y)\n",
    "# print(\" \")\n",
    "# img_logistic_regression_with_cv(tr_X.reshape((tr_X.shape[0], 68*2)), tr_Y)\n",
    "# print(\" \")\n",
    "# img_logistic_regression_with_randomized_search(tr_X.reshape((tr_X.shape[0], 136)), tr_Y)\n",
    "# print(\" \")\n",
    "# img_random_forest_with_cv(tr_X.reshape((tr_X.shape[0], 68*2)), tr_Y)\n",
    "# print(\" \")\n",
    "# img_random_forest_with_randomized_search(tr_X.reshape((tr_X.shape[0], 136)), tr_Y)\n",
    "# print(\" \")\n",
    "# img_decision_tree_with_cv(tr_X.reshape((tr_X.shape[0], 68*2)), tr_Y)\n",
    "# print(\" \")\n",
    "# img_decision_tree_with_randomized_search(tr_X.reshape((tr_X.shape[0], 136)), tr_Y)\n",
    "# print(\" \")\n",
    "# img_mlp_with_cv(tr_X.reshape((tr_X.shape[0], 68*2)), tr_Y)\n",
    "# print(\" \")\n",
    "# img_KNN_with_randomized_search(tr_X.reshape((tr_X.shape[0], 136)), tr_Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2e69689cd1625636f0c1dfd315897165b04a4e2851d2ac68933d3dd1db0adbc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
